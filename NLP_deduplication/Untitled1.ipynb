{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba2a06-e141-4023-b308-01665a2d51df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9f677a-6dd3-44a6-8dc6-52ae0b6ff23b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Device Name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Print the GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Device Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU instead.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9650539e-3f30-4c78-91b4-4ef7b013b654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fsspec==2023.3.0\n",
      "  Obtaining dependency information for fsspec==2023.3.0 from https://files.pythonhosted.org/packages/4f/65/887925f1549fcb6ac3abb23a747c10f5ab083e8471fe568768b18bdb15b2/fsspec-2023.3.0-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2023.3.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Using cached fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "Installing collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "Successfully installed fsspec-2023.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "huggingface-hub 0.26.2 requires fsspec>=2023.5.0, but you have fsspec 2023.3.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install fsspec==2023.3.0 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "035a846d-97a6-49a2-a30f-3a641cc1fdfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/c6/b2/454d6e7f0158951d8a78c2e1eb4f69ae81beb8dca5fee9809c6c99e9d0d0/fsspec-2024.10.0-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tanuj\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.3.0\n",
      "    Uninstalling fsspec-2023.3.0:\n",
      "      Successfully uninstalled fsspec-2023.3.0\n",
      "Successfully installed fsspec-2024.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.3.0 requires fsspec==2023.3.0, but you have fsspec 2024.10.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96d1a9b9-1c7a-4c35-aa18-4334a10d7e44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanuj\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:345: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|                                                           | 0/86519 [00:00<?, ?sentence/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BertTokenizer._tokenize() got an unexpected keyword argument 'truncation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Get embeddings for the sentences with a progress bar\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m get_embeddings(sentences)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Filter out near-duplicate sentences with a progress bar\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltering near-duplicate sentences...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 24\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tqdm(sentences, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# Tokenize and prepare input tensors using encode_plus\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m     25\u001b[0m             sentence,\n\u001b[0;32m     26\u001b[0m             return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     27\u001b[0m             truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     28\u001b[0m             padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m             max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m\n\u001b[0;32m     30\u001b[0m         )\n\u001b[0;32m     31\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;66;03m# Get embeddings (using CLS token representation)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:786\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 786\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    787\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(first_ids,\n\u001b[0;32m    790\u001b[0m                               pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m    791\u001b[0m                               max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m                               truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m    795\u001b[0m                               return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:778\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, six\u001b[38;5;241m.\u001b[39mstring_types):\n\u001b[1;32m--> 778\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], six\u001b[38;5;241m.\u001b[39mstring_types):\n\u001b[0;32m    780\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(token, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \\\n\u001b[0;32m    645\u001b[0m             \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens \\\n\u001b[0;32m    646\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m [token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized_text), [])\n\u001b[0;32m    648\u001b[0m added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens\n\u001b[1;32m--> 649\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m split_on_tokens(added_tokens, text)\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:644\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize.<locals>.split_on_tokens\u001b[1;34m(tok_list, text)\u001b[0m\n\u001b[0;32m    641\u001b[0m             tokenized_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [sub_text]\n\u001b[0;32m    642\u001b[0m     text_list \u001b[38;5;241m=\u001b[39m tokenized_text\n\u001b[1;32m--> 644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(token, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \\\n\u001b[0;32m    645\u001b[0m         \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens \\\n\u001b[0;32m    646\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m [token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized_text), [])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:644\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    641\u001b[0m             tokenized_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [sub_text]\n\u001b[0;32m    642\u001b[0m     text_list \u001b[38;5;241m=\u001b[39m tokenized_text\n\u001b[1;32m--> 644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(token, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \\\n\u001b[0;32m    645\u001b[0m         \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens \\\n\u001b[0;32m    646\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m [token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized_text), [])\n",
      "\u001b[1;31mTypeError\u001b[0m: BertTokenizer._tokenize() got an unexpected keyword argument 'truncation'"
     ]
    }
   ],
   "source": [
    "# Initialize mBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
    "\n",
    "# Function to get embeddings for a list of sentences with tqdm progress bar\n",
    "def get_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentence in tqdm(sentences, desc=\"Generating embeddings\", unit=\"sentence\"):\n",
    "            # Tokenize and prepare input tensors using encode_plus\n",
    "            inputs = tokenizer.encode_plus(\n",
    "                sentence,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=128\n",
    "            )\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "            # Get embeddings (using CLS token representation)\n",
    "            outputs = model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(cls_embedding.flatten())\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Function to filter near-duplicate sentences based on cosine similarity with tqdm\n",
    "def filter_duplicates(sentences, embeddings, threshold=0.75):\n",
    "    filtered_sentences = []\n",
    "    filtered_embeddings = []\n",
    "\n",
    "    for i, embedding in tqdm(enumerate(embeddings), desc=\"Filtering duplicates\", total=len(embeddings)):\n",
    "        # Calculate cosine similarity with already accepted embeddings\n",
    "        if len(filtered_embeddings) == 0:\n",
    "            filtered_sentences.append(sentences[i])\n",
    "            filtered_embeddings.append(embedding)\n",
    "            continue\n",
    "\n",
    "        similarities = cosine_similarity([embedding], filtered_embeddings)[0]\n",
    "        max_similarity = max(similarities)\n",
    "\n",
    "        # Only add the sentence if similarity is below the threshold\n",
    "        if max_similarity < threshold:\n",
    "            filtered_sentences.append(sentences[i])\n",
    "            filtered_embeddings.append(embedding)\n",
    "\n",
    "    return filtered_sentences\n",
    "\n",
    "# Load your sentences (Updated column name to \"Sentences\")\n",
    "data = pd.read_csv(\"syntactic_filtered_sentences.csv\")\n",
    "sentences = data['Sentences'].tolist()\n",
    "\n",
    "# Get embeddings for the sentences with a progress bar\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = get_embeddings(sentences)\n",
    "\n",
    "# Filter out near-duplicate sentences with a progress bar\n",
    "print(\"Filtering near-duplicate sentences...\")\n",
    "filtered_sentences = filter_duplicates(sentences, embeddings, threshold=0.75)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Original number of sentences: {len(sentences)}\")\n",
    "print(f\"Number of unique sentences after filtering: {len(filtered_sentences)}\")\n",
    "\n",
    "# Save the filtered sentences to a CSV file\n",
    "filtered_df = pd.DataFrame(filtered_sentences, columns=[\"Sentences\"])\n",
    "filtered_df.to_csv(\"filtered_sentences.csv\", index=False)\n",
    "print(\"Filtered sentences saved to 'filtered_sentences.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f38070-ee4c-4956-b0b9-938870bdf4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
